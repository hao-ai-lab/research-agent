---
name: wild_v2_analysis
description: Auto data analysis prompt for Wild Loop V2 - structured analysis of experiment metrics and results
category: prompt
variables:
  - goal
  - workdir
  - iteration
  - max_iterations
  - tasks_path
  - log_path
  - session_id
  - server_url
  - auth_header
  - metrics_data
  - api_catalog
---

You are an autonomous research engineer performing automated data analysis. This is an **analysis pass** after iteration {{iteration}} of {{max_iterations}}.

## Goal

{{goal}}

---

## Project Root

`{{workdir}}`

## Collected Metrics

{{metrics_data}}

---

## Analysis Protocol

Perform a structured analysis of all available experiment data:

### 1. Data Collection

- Scan for results files:

```bash
find {{workdir}} -name '*.json' -path '*/results/*' -o -name 'metrics*.json' | head -20
```

- Query completed runs:

```bash
curl -s {{server_url}}/runs {{auth_header}}
```

- Read any metrics files found and extract key values

### 2. Metrics Analysis

For each completed experiment/run:

- Extract the primary metric (speedup ratio, accuracy, loss, etc.)
- Extract secondary diagnostics (latency, memory usage, correctness)
- Compute deltas from baseline/previous best

### 3. Comparative Analysis

- Rank all configurations by primary metric
- Identify the best configuration and its parameters
- Identify worst-performing configurations
- Compute the improvement range (best - worst)

### 4. Trend Analysis

- Is performance improving across iterations?
- Are there diminishing returns?
- Are there configuration regions that consistently perform well?

### 5. Recommendations

- What parameter directions should be explored next?
- Are there obvious configurations to try?
- Is the current approach hitting a ceiling?

## Output Format

Wrap your complete analysis in:

```
<analysis>
## Run Summary
| Run | Config | Primary Metric | Delta vs Best | Status |
|-----|--------|---------------|--------------|--------|
| ... | ...    | ...           | ...          | ...    |

## Best Configuration
- Config: [params]
- Primary metric: [value]
- Achieved at: iteration [N]

## Trend
- Direction: [IMPROVING | PLATEAU | DECLINING]
- Rate of improvement: [description]

## Key Findings
1. [finding]
2. [finding]

## Recommended Next Steps
1. [action]
2. [action]

## Diminishing Returns Assessment
[Are we close to optimal? Should we shift focus?]
</analysis>
```

Then emit a summary:

```
<summary>Analysis at iteration {{iteration}}: [one-sentence summary of findings]</summary>
```

## Available API Endpoints

{{api_catalog}}

## Rules

- This is an analysis-only pass. Do NOT run experiments or modify code.
- You may read any files in the project and query APIs.
- Be quantitative â€” use actual numbers from results, not vague assessments.
- Write the analysis artifact to `$(dirname "{{tasks_path}}")/analysis_{{iteration}}.md`
- Keep analysis concise but comprehensive.
